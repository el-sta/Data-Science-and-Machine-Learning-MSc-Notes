{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the width of the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e71ce",
   "metadata": {},
   "source": [
    "### This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df72df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7), \"This script requires Python 3.7 or higher!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ab795",
   "metadata": {},
   "source": [
    "### It also requires Scikit-Learn ≥ 1.0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a28699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path # Use pathlib.Path for cleaner, object-oriented, and cross-platform file path handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb215b3",
   "metadata": {},
   "source": [
    "### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9f5967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "print(iris.keys())        #dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
    "print(iris.target_names)  #['setosa' 'versicolor' 'virginica']\n",
    "print(iris.feature_names) #['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23367c6d",
   "metadata": {},
   "source": [
    "### MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70272f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST dataset and check the keys of the mnist object\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "mnist.keys()\n",
    "\n",
    "# split the data into features and target values\n",
    "X = mnist.iloc[:, :-1]  # Features\n",
    "y = mnist.iloc[:, -1]   # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3be74",
   "metadata": {},
   "source": [
    "### Fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a784c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    " # Load dataset\n",
    "(x_train_full, y_train_full), (x_test_full, y_test_full) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4414919",
   "metadata": {},
   "source": [
    "### Take a Quick Look at the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()                                  #look at the top five rows of data\n",
    "data.info()                                  #get a quick description of the data\n",
    "data.describe()                              #shows a summary of the numerical attributes\n",
    "data[\"column_name\"].value_counts()           #for categorical attribute (object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "housing.hist(bins=50, figsize=(12, 8))       #call the hist() method on the whole dataset and it will plot a histogram \n",
    "plt.show()                                   #for each numericaattribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812da02f",
   "metadata": {},
   "source": [
    "### Create a Test Set 80% and 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#most common when you already have your X (features) and y (labels) split\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,                              #ensures class balance in the train/test sets\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------or------------------------------------------------------------------\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,                           #Split the dataset into a training and a testing set retaining 80% and 20%\n",
    "    stratify=data[\"target_column\"],                              \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "# X includes every column except the last one (quality) and y includes only the last\n",
    "X_train = train_set.iloc[:, :-1]  \n",
    "y_train = train_set.iloc[:, -1] \n",
    "X_test = test_set.iloc[:, :-1]    \n",
    "y_test = test_set.iloc[:, -1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961abcfe",
   "metadata": {},
   "source": [
    "### training (5/7), a validation (1/7), and a test (1/7) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(np.uint8)\n",
    "\n",
    "# Step 1: split into 5/7 train and 2/7 temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=2/7, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: split temp into 1/7 val and 1/7 test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Confirm shapes\n",
    "print(\"Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a0c8e",
   "metadata": {},
   "source": [
    "### Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b82aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most machine learning algorithms cannot work with missing features, so you’ll need to take care of these\n",
    " #1. Get rid of the corresponding districts.\n",
    " #2. Get rid of the whole attribute.\n",
    " #3. Set the missing values to some value (zero, the mean, the median,etc.). This is called imputation.\n",
    "#You can accomplish these easily using the Pandas DataFrame’s dropna(), drop(), and fillna() methods:\n",
    "\n",
    "data.dropna(subset=[\"column_name\"], inplace=True)     # option 1 Delete all the rows with NaN\n",
    "data.drop(\"column_name\", axis=1)                      # option 2 Delete the entire column\n",
    "median = data[\"column_name\"].median()                 # option 3 Replace all NaN with the median or sklearn.SimpleImputer\n",
    "data[\"column_name\"].fillna(median, inplace=True)\n",
    "\n",
    "#SimpleImpute: The benefit is that it will store the median value of each feature.\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "data_num = data.select_dtypes(include=[np.number])    #median can only be computed on numerical attributes\n",
    "imputer.fit(data_num)\n",
    "imputer.statistics_\n",
    "data_num.median().values\n",
    "X = imputer.transform(data_num)\n",
    "#( There are also more powerful imputers available in the sklearn.impute package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1f6b5",
   "metadata": {},
   "source": [
    "### Feature Scaling and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One of the most important transformations you need to apply to your data is feature scaling.\n",
    "#There are two common ways to get all attributes to have the same scale:min-max scaling(normalization) and standardization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler            \n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)           #use fit() or fit_transform() to the training data only\n",
    "X_test_scaled = scaler.transform(X_test)                 #use transform() to the validation set, the test set, and new data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)          #Fit only to training set\n",
    "X_valid_scaled = scaler.transform(X_valid)              #Transform\n",
    "X_test_scaled  = scaler.transform(X_test)               #Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e3a3d",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=0.80)  \n",
    ")\n",
    "x_pca = pca_pipeline.fit_transform(x_flatten)\n",
    "\n",
    "#or\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components=0.80)                           #preserving 80% of the variance\n",
    "x_pca = pca.fit_transform(x_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Results\n",
    "# Determine the number of principal components\n",
    "num_pca_components = pca.n_components_\n",
    "print(f\"Number of PCA components to preserve 80% variance: {pca.n_components_}\")\n",
    "\n",
    "# Transform the original data using these principal components\n",
    "Data1 = x_pca\n",
    "\n",
    "print(f\"Data1 training data shape: {Data1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982cb95",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "scaler = StandardScaler()                              # Scale the input features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lin_reg_model = LinearRegression()                     # Create the Linear Regression model\n",
    "\n",
    "lin_reg_model.fit(X_train_scaled, y_train)             # Fit the model to training data\n",
    "\n",
    "y_pred_lin = lin_reg_model.predict(X_test_scaled)      # Predict on the test set\n",
    "\n",
    "print(\"Linear Regression:\")                            # Evaluate the model\n",
    "print(f\"RMSE: {mean_squared_error(y_test, y_pred_lin, squared=False):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred_lin):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff186f",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scaler = StandardScaler()                              # Scale the input features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556e454",
   "metadata": {},
   "source": [
    "### SGD(Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838f184",
   "metadata": {},
   "source": [
    "### SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a09586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "scaler = StandardScaler()                                 # Scale the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=0.1, random_state=42)  # Create the SVM model\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train)                    # Train the model\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)             # Make predictions\n",
    "\n",
    "print(\"SVM Classifier (Linear Kernel):\")                  # Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.2f}\")\n",
    "print(f\"F1 Score (macro): {f1_score(y_test, y_pred_svm, average='macro'):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b324758",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier/Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier         # -->  classification, predicts a category or class label\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create the classifier\n",
    "clf_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on training data\n",
    "clf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_clf = clf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Decision Tree Classifier:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_clf):.2f}\")\n",
    "print(f\"F1 Score (macro): {f1_score(y_test, y_pred_clf, average='macro'):.2f}\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor         # -->  regression, predicts a numerical (continuous) value\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create the regressor\n",
    "reg_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Fit the model on training data\n",
    "reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_reg = reg_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Decision Tree Regressor:\")\n",
    "print(f\"RMSE: {mean_squared_error(y_test, y_pred_reg, squared=False):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred_reg):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66601d8b",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1156de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define PCA\n",
    "pca = PCA(random_state = 42, n_components = clf[0].n_components_)\n",
    "\n",
    "# Define GBC model\n",
    "gbrt_clf = GradientBoostingClassifier(max_depth = 2,\n",
    "                                      n_estimators = 6,\n",
    "                                      learning_rate = 1.0,\n",
    "                                      random_state = 42)\n",
    "\n",
    "# Transform input data\n",
    "X_train_transformed = pca.fit_transform(X_train)\n",
    "\n",
    "# Fit model\n",
    "gbrt_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = gbrt_clf.predict(pca.transform(X_test))\n",
    "\n",
    "print(\"PCA + Gradient Boosting Classifier:\")\n",
    "print(f\"Accuracy score: {accuracy_score(y_test,y_pred):.2f}\")\n",
    "print(f\"F1: {f1_score(y_test,y_pred, average= 'macro'):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e810d",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define pipeline: (scaling is optional for Random Forest, but included for consistency)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),                       # Optional for RF, but good in pipelines\n",
    "    ('rf', RandomForestClassifier(n_estimators=100,     # Number of trees\n",
    "                                  max_depth=10,         # Optional limit on tree depth\n",
    "                                  random_state=42))     # For reproducibility\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"F1 Score (macro): {f1_score(y_test, y_pred, average='macro'):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec3a70",
   "metadata": {},
   "source": [
    "### Bagging classifier (ensemble method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM model with a linear kernel inside a pipeline with StandardScaler\n",
    "from sklearn.svm import SVC                             # use any classifier from scikit-learn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm_pipeline = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", random_state=42))\n",
    "# Create a Bagging Classifier with 10 estimators using the SVM pipeline\n",
    "bag_clf = BaggingClassifier(\n",
    "    estimator=svm_pipeline,  \n",
    "    n_estimators=10,\n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "# Train the Bagging model\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1229b",
   "metadata": {},
   "source": [
    "### AdaBoost classifier(ensemble method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define a pipeline with optional scaling and AdaBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),                           # Optional: useful for models sensitive to feature scale\n",
    "    ('ada', AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=1), # Weak learner (decision stump)\n",
    "        n_estimators=100,                                   # Number of boosting rounds\n",
    "        learning_rate=0.25,                                 # Step size for each estimator\n",
    "        random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"F1 Score (macro): {f1_score(y_test, y_pred, average='macro'):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75586b1",
   "metadata": {},
   "source": [
    "### SelfTrainingClassifier (semi-supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "\n",
    "# Suppose you have some labels and some unknown (-1 indicates unknown label)\n",
    "y_train_semi = y_train.copy()\n",
    "y_train_semi[100:] = -1   # the first 100 rows have labels, the rest are unlabeled\n",
    "\n",
    "\n",
    "# Define the base classifier\n",
    "base_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Wrap it with SelfTrainingClassifier for semi-supervised learning\n",
    "semi_supervised_clf = SelfTrainingClassifier(\n",
    "    base_estimator=base_clf,\n",
    "    criterion=\"threshold\",\n",
    "    threshold=0.99\n",
    ")\n",
    "\n",
    "# Fit on labeled + unlabeled data\n",
    "# y_train_semi must have -1 for unlabeled instances\n",
    "semi_supervised_clf.fit(X_train, y_train_semi)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = semi_supervised_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5aa35",
   "metadata": {},
   "source": [
    "### Grid Search(hyperparameter tuning technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Train the model here\n",
    "clf\n",
    "\n",
    "# Parameters for Grid Search\n",
    "params = {'max_depth': [], 'max_features' : []}\n",
    "\n",
    "# Perform Grid Search and train the model\n",
    "clf2 = GridSearchCV(estimator = clf, param_grid = params)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "y_pred = clf2.best_estimator_.predict(X_test)\n",
    "\n",
    "best_params = clf2.best_estimator_.get_params()\n",
    "\n",
    "print(\"Decision Tree Classifier:\")\n",
    "print(f\"Best Max Depth: {best_params['max_depth']}\")\n",
    "print(f\"Best Max Features: {best_params['max_features']}\")\n",
    "print(f\"Accuracy score: {accuracy_score(y_test,y_pred):.2f}\")\n",
    "print(f\"F1: {f1_score(y_test,y_pred, average= 'macro'):.2f}\")\n",
    "print()\n",
    "\n",
    "for i_conf, conf in enumerate(clf2.cv_results_['params']):\n",
    "    print(f\"Configuration: {conf}  Mean Test Score: {clf2.cv_results_['mean_test_score'][i_conf]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86acf9",
   "metadata": {},
   "source": [
    "### Grid Search with cross validation and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Define pipeline: scaler + model\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "# 2. Define parameter grid for GridSearchCV (use step names!)\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# 3. Grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839763d1",
   "metadata": {},
   "source": [
    "### KMeans (clustering unsupervised learning algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff32068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 1. Scale the input features (important for clustering)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Create the KMeans model\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=42, n_init='auto')  # You can change n_clusters as needed\n",
    "\n",
    "# 3. Fit the model and predict cluster labels\n",
    "y_kmeans = kmeans_model.fit_predict(X_scaled)\n",
    "\n",
    "# 4. Evaluate clustering performance\n",
    "silhouette = silhouette_score(X_scaled, y_kmeans)\n",
    "print(\"K-Means Clustering:\")\n",
    "print(f\"Number of clusters: {kmeans_model.n_clusters}\")\n",
    "print(f\"Silhouette Score: {silhouette:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54927cc7",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbcbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A pipeline should be built, instead of using the scaled data as input, in order to avoid information \"leakage\" during\n",
    "#formation of the different folds. A pipeline can be formed using either the \"Pipeline\" method, where naming of each \n",
    "#stage is performed by the user or the \"make_pipeline\" method which automatically names the pipeline stages.\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "lin_reg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", LinearRegression())\n",
    "    ])\n",
    "\n",
    "k_folds = KFold(n_splits = 10)\n",
    "scores = cross_val_score(lin_reg, x_train, y_train, cv = k_folds) \n",
    "\n",
    "print(f\"Cross Validation Scores: {[np.round(s, 4) for s in scores]}\")\n",
    "print(f\"Average CV Score: {scores.mean():.4f}, with STD: {scores.std():.4f}\")\n",
    "print(f\"Number of CV Scores used in Average: {len(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48780d4e",
   "metadata": {},
   "source": [
    "### Classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932878a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred_dtr))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444d35a",
   "metadata": {},
   "source": [
    "### Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d11d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_train, y_train_pred)            #Calculate the confusion matrix for the training set\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7c39d",
   "metadata": {},
   "source": [
    "### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the F1-score for classifier clf\n",
    "#To calculate F1-score, you need to have y_pred, which means you must have used predict().\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = clf.predict(X_scaled)\n",
    "f1 = f1_score(y, y_pred)\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c27cae",
   "metadata": {},
   "source": [
    "### Αccuracy-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the accuracy-score for classifier clf\n",
    "#To calculate accuracy, you need to have y_pred, which means you must have used predict().\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_scaled)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy with respect to test data:{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601eb869",
   "metadata": {},
   "source": [
    "## Tensorflow.Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input,Dense, Dropout, Flatten, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds for TensorFlow, Python (random.seed()), and NumPy (np.random.seed())\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d898ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset using Keras’ dataset module\n",
    " fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fcd490",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00939ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Model creation function\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(14, 28)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "initial_weights = model.get_weights()  # Store the random initial weights\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22516387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and store history\n",
    "history = model.fit(\n",
    "    X_train_upper, y_train,\n",
    "    validation_data=(X_valid_upper, y_valid),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcce2b",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ec478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Apply Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True  # Restore best weights after stopping\n",
    ")\n",
    "# Train with Early Stopping\n",
    " history_early = model.fit(\n",
    "    X_train_upper, y_train,\n",
    "    validation_data=(X_valid_upper, y_valid),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753cc62",
   "metadata": {},
   "source": [
    "###  BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6130ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Input(shape=[14, 28]),\n",
    "                    Flatten(),\n",
    "                    Dense(128, use_bias=False),\n",
    "                    BatchNormalization(),\n",
    "                    Activation(\"relu\"),\n",
    "                    Dense( 64, use_bias=False),\n",
    "                    BatchNormalization(),\n",
    "                    Activation(\"relu\"),\n",
    "                    Dense( 32, use_bias=False),\n",
    "                    BatchNormalization(),\n",
    "                    Activation(\"relu\"),\n",
    "                    Dense(10, activation=\"softmax\")])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce193e",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Input(shape=[14, 28]),\n",
    "                    Flatten(),\n",
    "                    Dropout(rate=0.50),\n",
    "                    Dense(128, activation = \"relu\"),\n",
    "                    Dropout(rate=0.50),\n",
    "                    Dense( 64, activation = \"relu\"),\n",
    "                    Dropout(rate=0.50),\n",
    "                    Dense( 32, activation = \"relu\"),\n",
    "                    Dropout(rate=0.50),\n",
    "                    Dense( 10, activation = \"softmax\")])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766c82a",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Build the Convolutional Neural Network (CNN) using TensorFlow's Keras API\n",
    "model = tf.keras.models.Sequential([   # Creates a linear stack of layers, each layer feeds directly into the next\n",
    "    # Feature extractor\n",
    "    tf.keras.Input(shape=(28, 28, 1)), # Input images: 28×28 pixels, 1 channel (grayscale)\n",
    "    layers.Conv2D(8, (5, 5), strides=(2, 2), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Flatten(), # Converts the 3D output of convolutions into a 1D vector for the classifier\n",
    "    # Classifier\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax') # softmax turns outputs into probabilities summing to 1\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    " model.compile(optimizer='adam',                    # Adam Optimizer\n",
    "              loss='categorical_crossentropy',      # categorical_crossentropy for one-hot encoded labels\n",
    "              metrics=['accuracy'])   \n",
    " #  Model summary\n",
    " model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e24f55",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc80ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.set(style=\"whitegrid\") \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df, x='', y='', label='', color='blue')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('')\n",
    "plt.title('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "look_back = 30\n",
    "forecast_horizon = 1  # Predict one step ahead\n",
    "\n",
    "# Reshape the data into a column vector\n",
    "sunspots_array = sunspots_array.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train/test sets (90% train)\n",
    "train_size = int(len(sunspots_array) * 0.9)\n",
    "train_data = sunspots_array[:train_size]\n",
    "test_data = sunspots_array[train_size:]\n",
    "\n",
    "# Normalize separately to avoid data leakage\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Flatten the arrays for compatibility with sequence preparation\n",
    "train_data_scaled = train_data_scaled.flatten()\n",
    "test_data_scaled = test_data_scaled.flatten()\n",
    "\n",
    "# Sequence-to-vector function\n",
    "def create_seq2vec_dataset(data, look_back, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Converts a time series into supervised learning format:\n",
    "    One input sample of `look_back` time steps predicts `forecast_horizon` steps ahead.\n",
    "    \"\"\"\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(data) - look_back - forecast_horizon + 1):\n",
    "        input_seq = data[i:i + look_back]\n",
    "        output_seq = data[i + look_back:i + look_back + forecast_horizon]\n",
    "        dataX.append(input_seq)\n",
    "        dataY.append(output_seq)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# Example usage:\n",
    "X_train, y_train = create_seq2vec_dataset(train_data_scaled, look_back, forecast_horizon)\n",
    "X_test, y_test = create_seq2vec_dataset(test_data_scaled, look_back, forecast_horizon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c244c",
   "metadata": {},
   "source": [
    "###  Multilayer Percepton Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e59d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, BatchNormalization, Dense\n",
    "from keras.optimizers import AdamW\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "output_size= y_target.shape[1] \n",
    "\n",
    "# Define model\n",
    "model = Sequential([                                                  # Initializes a Sequential model\n",
    "    Input(shape=(look_back,)),                                        # Input layer with shape = look_back\n",
    "    BatchNormalization(),                                             # Batch Norm layer\n",
    "    Dense(50, activation='selu', kernel_initializer='lecun_normal'),  # Dense(50)\n",
    "    Dense(25, activation='selu', kernel_initializer='lecun_normal'),  # Dense(25)\n",
    "    Dense(output_size)                                                # Οutput layer\n",
    "])                                              \n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=AdamW(learning_rate=0.001),                            # Optimizer with decoupled weight decay\n",
    "    loss='mse',                                                      # Mean Squared Error for regression\n",
    "    metrics=['mae']                                                  # Mean Absolute Error as a metric\n",
    ")                                                                    \n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',                                              # Monitor validation loss\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_input,\n",
    "    y_target,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,                                            # Use 10% of data for validation\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed612e5",
   "metadata": {},
   "source": [
    "### Stacked Autoencoder (for MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define input shape and parameters\n",
    "input_shape = (28, 28)                                # Input is a 28x28 image (e.g. from MNIST)\n",
    "flatten_shape = 28 * 28                               # Flattened shape = 784\n",
    "latent_dim = 50                                       # Size of the encoded (compressed) representation\n",
    "\n",
    "# Function to build the autoencoder model\n",
    "def build_autoencoder():\n",
    "    # Encoder network\n",
    "    encoder = models.Sequential([\n",
    "        tf.keras.Input(shape=input_shape),            # Input layer (28x28 image)\n",
    "        layers.Flatten(),                             # Flatten image to vector of size 784\n",
    "        layers.Dense(256, activation='relu'),         # Dense hidden layer\n",
    "        layers.Dense(latent_dim, activation='relu', name='latent')  # Encoded representation (latent space)\n",
    "    ])\n",
    "\n",
    "    # Decoder network\n",
    "    decoder = models.Sequential([\n",
    "        tf.keras.Input(shape=(latent_dim,)),          # Input is latent vector (size 50)\n",
    "        layers.Dense(256, activation='relu'),         # Hidden layer\n",
    "        layers.Dense(flatten_shape, activation='sigmoid'),  # Output layer with sigmoid activation (range 0–1)\n",
    "        layers.Reshape(input_shape)                   # Reshape back to 28x28 image\n",
    "    ])\n",
    "\n",
    "    # Combine encoder and decoder into a single model\n",
    "    autoencoder = models.Sequential([encoder, decoder])\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(\n",
    "        optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-4),  # Nadam optimizer with small learning rate\n",
    "        loss='binary_crossentropy'                                # Binary cross-entropy loss for pixel-wise comparison\n",
    "    )\n",
    "    return autoencoder\n",
    "\n",
    "# Build and summarize the model\n",
    "autoencoder = build_autoencoder()\n",
    "autoencoder.summary()\n",
    "\n",
    "# Save the autoencoder weights\n",
    "autoencoder_weights = autoencoder.get_weights()  # List of all trainable weights in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6034c3",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e03554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alias for Dense layer\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "# Size of the noise vector (latent space)\n",
    "codings_size = 30\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 32\n",
    "\n",
    "# Define the Generator model\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(codings_size,)),               # Input: random noise vector of size 30\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),   # Dense layer with 100 units and ReLU activation\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),   # Dense layer with 150 units and ReLU activation\n",
    "    Dense(28 * 28, activation=\"tanh\"),                                # Output layer to produce 784 values in [-1, 1]\n",
    "    tf.keras.layers.Reshape([28, 28])                                 # Reshape to image format (28x28)\n",
    "])\n",
    "generator.summary()\n",
    "\n",
    "# Define the Discriminator model\n",
    "discriminator = Sequential([\n",
    "    InputLayer(shape=(28, 28)),                                       # Input: 28x28 image\n",
    "    Flatten(),                                                        # Flatten image to vector\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),    # Dense layer with 150 units\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),    # Dense layer with 100 units\n",
    "    Dense(1, activation=\"sigmoid\")                                    # Output layer: probability that input is real\n",
    "])\n",
    "discriminator.summary()\n",
    "\n",
    "# Compile the Discriminator (standalone) with training enabled\n",
    "discriminator.trainable = True\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# Freeze Discriminator's weights when training the combined GAN\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Define the combined GAN model: Generator followed by Discriminator\n",
    "gan = tf.keras.Sequential([generator, discriminator])\n",
    "\n",
    "# Compile the full GAN model\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# Create a tf.data.Dataset from X_train and prepare it for training\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)       # Shuffle data\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)      # Batch and prefetch for performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for X_batch in dataset:\n",
    "            #Enable discriminator training\n",
    "            discriminator.trainable = True\n",
    "            # Phase 1 train discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "            y1 = tf.concat([\n",
    "                tf.zeros((batch_size, 1)),\n",
    "                tf.ones((batch_size, 1)) * 0.9\n",
    "            ], axis=0)\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            #Freeze discriminator for GAN training (generator only)\n",
    "            discriminator.trainable = False\n",
    "            # Phase 2 train generator via GAN\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.ones((batch_size, 1))  # trick discriminator: all real for generator training\n",
    "            gan.train_on_batch(noise, y2)\n",
    "        # Plot generated images after each epoch\n",
    "        plot_multiple_images(generated_images.numpy(), 32)\n",
    "        \n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
